{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff92f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (6300, 59) (630, 59)\n",
      "\n",
      "Tuning XGB...\n",
      "Best XGB: {'colsample_bytree': np.float64(0.8834959481464842), 'learning_rate': np.float64(0.01035331526098587), 'max_depth': 3, 'n_estimators': 198, 'subsample': np.float64(0.8574323980775167)}\n",
      "\n",
      "Tuning LGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000787 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2797\n",
      "[LightGBM] [Info] Number of data points in the train set: 6300, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 2511.639524\n",
      "Best LGBM: {'colsample_bytree': np.float64(0.8703100983459974), 'learning_rate': np.float64(0.01156566462277793), 'n_estimators': 367, 'num_leaves': 63, 'subsample': np.float64(0.9818496824692566)}\n",
      "\n",
      "Tuning RF...\n",
      "Best RF: {'max_depth': 21, 'min_samples_leaf': 3, 'n_estimators': 443}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaish\\Downloads\\crime-prediction\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\vaish\\Downloads\\crime-prediction\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching best weights...\n",
      "\n",
      "============================\n",
      "ðŸ”¥ Best Hybrid Weights:\n",
      "XGB : 0.1\n",
      "LGBM: 0.1\n",
      "RF  : 0.8\n",
      "============================\n",
      "Hybrid RÂ²   : 0.7847280395040487\n",
      "Hybrid RMSE : 3256.465781789071\n",
      "Hybrid MAE  : 976.934608631893\n",
      "============================\n",
      "\n",
      "================ TRAIN ERRORS ================\n",
      "\n",
      "ðŸ“Œ XGBoost Train Metrics\n",
      "RÂ²   : 0.6522330045700073\n",
      "RMSE : 4815.791316076726\n",
      "MAE  : 1488.5692138671875\n",
      "\n",
      "ðŸ“Œ LightGBM Train Metrics\n",
      "RÂ²   : 0.8404410038130484\n",
      "RMSE : 3262.001333680626\n",
      "MAE  : 827.0891438850267\n",
      "\n",
      "ðŸ“Œ Random Forest Train Metrics\n",
      "RÂ²   : 0.880835801807937\n",
      "RMSE : 2819.0084918764833\n",
      "MAE  : 617.6956034890508\n",
      "\n",
      "ðŸ”¥ Hybrid Model Train Metrics\n",
      "RÂ²   : 0.8641112371049979\n",
      "RMSE : 3010.3379875919504\n",
      "MAE  : 695.7081712066962\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# ============================================\n",
    "# 1. Load DATA\n",
    "# ============================================\n",
    "df = pd.read_csv(\"../data/cleaned/final_merged_dataset.csv\")\n",
    "\n",
    "target = \"Cases\"\n",
    "features = [\n",
    "    'State', 'Crime_Type', 'Year', 'unemployment_rate', 'poverty_rate', 'per_capita_income',\n",
    "    'inflation_rate', 'population_density', 'gender_ratio', 'literacy_rate',\n",
    "    'youth_population_percent', 'urbanization_rate', 'human_development_index',\n",
    "    'police_stations_per_district', 'conviction_rate', 'police_personnel_per_100k',\n",
    "    'alcohol_consumption_per_capita'\n",
    "]\n",
    "\n",
    "numerical_cols = [\n",
    "    'Year', 'unemployment_rate', 'poverty_rate', 'per_capita_income',\n",
    "    'inflation_rate', 'population_density', 'gender_ratio', 'literacy_rate',\n",
    "    'youth_population_percent', 'urbanization_rate', 'human_development_index',\n",
    "    'police_stations_per_district', 'conviction_rate',\n",
    "    'police_personnel_per_100k', 'alcohol_consumption_per_capita'\n",
    "]\n",
    "\n",
    "categorical_cols = ['State', 'Crime_Type']\n",
    "\n",
    "# time-split\n",
    "train_df = df[df[\"Year\"] <= 2020]\n",
    "test_df  = df[df[\"Year\"] > 2020]\n",
    "\n",
    "y_train = train_df[target]\n",
    "y_test  = test_df[target]\n",
    "\n",
    "# ============================================\n",
    "# 2. Encoding + Scaling\n",
    "# ============================================\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "])\n",
    "\n",
    "X_train = preprocessor.fit_transform(train_df[features])\n",
    "X_test  = preprocessor.transform(test_df[features])\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "# ============================================\n",
    "# 3. FAST RANDOMIZED TUNING\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nTuning XGB...\")\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "xgb_params = {\n",
    "    \"n_estimators\": randint(150, 500),\n",
    "    \"max_depth\": randint(3, 8),\n",
    "    \"learning_rate\": uniform(0.01, 0.05),\n",
    "    \"subsample\": uniform(0.7, 0.3),\n",
    "    \"colsample_bytree\": uniform(0.7, 0.3)\n",
    "}\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb_model, xgb_params, n_iter=15, scoring='r2',\n",
    "    cv=3, random_state=42, n_jobs=-1\n",
    ")\n",
    "xgb_search.fit(X_train, y_train)\n",
    "xgb_best = xgb_search.best_estimator_\n",
    "print(\"Best XGB:\", xgb_search.best_params_)\n",
    "\n",
    "print(\"\\nTuning LGBM...\")\n",
    "lgb_model = LGBMRegressor(random_state=42)\n",
    "\n",
    "lgb_params = {\n",
    "    \"n_estimators\": randint(150, 500),\n",
    "    \"num_leaves\": randint(20, 80),\n",
    "    \"learning_rate\": uniform(0.01, 0.05),\n",
    "    \"subsample\": uniform(0.7, 0.3),\n",
    "    \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "}\n",
    "\n",
    "lgb_search = RandomizedSearchCV(\n",
    "    lgb_model, lgb_params, n_iter=15, scoring='r2',\n",
    "    cv=3, random_state=42, n_jobs=-1\n",
    ")\n",
    "lgb_search.fit(X_train, y_train)\n",
    "lgb_best = lgb_search.best_estimator_\n",
    "print(\"Best LGBM:\", lgb_search.best_params_)\n",
    "\n",
    "print(\"\\nTuning RF...\")\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": randint(200, 600),\n",
    "    \"max_depth\": randint(10, 30),\n",
    "    \"min_samples_leaf\": randint(1, 4)\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf_model, rf_params, n_iter=12, scoring='r2',\n",
    "    cv=3, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_search.fit(X_train, y_train)\n",
    "rf_best = rf_search.best_estimator_\n",
    "print(\"Best RF:\", rf_search.best_params_)\n",
    "\n",
    "# ============================================\n",
    "# 4. PREDICT BASE MODELS\n",
    "# ============================================\n",
    "\n",
    "pred_xgb_train = xgb_best.predict(X_train)\n",
    "pred_lgb_train = lgb_best.predict(X_train)\n",
    "pred_rf_train  = rf_best.predict(X_train)\n",
    "\n",
    "pred_xgb_test = xgb_best.predict(X_test)\n",
    "pred_lgb_test = lgb_best.predict(X_test)\n",
    "pred_rf_test  = rf_best.predict(X_test)\n",
    "\n",
    "# ============================================\n",
    "# 5. HYBRID WEIGHT SEARCH (Very Fast)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nSearching best weights...\")\n",
    "\n",
    "weights = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "best_r2 = -999\n",
    "best_combo = None\n",
    "best_test_pred = None\n",
    "\n",
    "for wx in weights:\n",
    "    for wl in weights:\n",
    "        wr = 1 - (wx + wl)\n",
    "        if wr <= 0: \n",
    "            continue\n",
    "\n",
    "        pred_t = wx*pred_xgb_train + wl*pred_lgb_train + wr*pred_rf_train\n",
    "        r2 = r2_score(y_train, pred_t)\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_combo = (wx, wl, wr)\n",
    "            best_test_pred = wx*pred_xgb_test + wl*pred_lgb_test + wr*pred_rf_test\n",
    "\n",
    "# ============================================\n",
    "# 6. FINAL METRICS\n",
    "# ============================================\n",
    "hybrid_r2 = r2_score(y_test, best_test_pred)\n",
    "hybrid_rmse = np.sqrt(np.mean((y_test - best_test_pred)**2))\n",
    "hybrid_mae = np.mean(np.abs(y_test - best_test_pred))\n",
    "\n",
    "print(\"\\n============================\")\n",
    "print(\"ðŸ”¥ Best Hybrid Weights:\")\n",
    "print(\"XGB :\", round(best_combo[0],2))\n",
    "print(\"LGBM:\", round(best_combo[1],2))\n",
    "print(\"RF  :\", round(best_combo[2],2))\n",
    "print(\"============================\")\n",
    "print(\"Hybrid RÂ²   :\", hybrid_r2)\n",
    "print(\"Hybrid RMSE :\", hybrid_rmse)\n",
    "print(\"Hybrid MAE  :\", hybrid_mae)\n",
    "print(\"============================\")\n",
    "\n",
    "# ============================================\n",
    "# 7. TRAIN METRICS FOR ALL MODELS\n",
    "# ============================================\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return r2, rmse, mae\n",
    "\n",
    "print(\"\\n================ TRAIN ERRORS ================\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_r2, xgb_rmse, xgb_mae = get_metrics(y_train, pred_xgb_train)\n",
    "print(\"\\nðŸ“Œ XGBoost Train Metrics\")\n",
    "print(\"RÂ²   :\", xgb_r2)\n",
    "print(\"RMSE :\", xgb_rmse)\n",
    "print(\"MAE  :\", xgb_mae)\n",
    "\n",
    "# LightGBM\n",
    "lgb_r2, lgb_rmse, lgb_mae = get_metrics(y_train, pred_lgb_train)\n",
    "print(\"\\nðŸ“Œ LightGBM Train Metrics\")\n",
    "print(\"RÂ²   :\", lgb_r2)\n",
    "print(\"RMSE :\", lgb_rmse)\n",
    "print(\"MAE  :\", lgb_mae)\n",
    "\n",
    "# Random Forest\n",
    "rf_r2, rf_rmse, rf_mae = get_metrics(y_train, pred_rf_train)\n",
    "print(\"\\nðŸ“Œ Random Forest Train Metrics\")\n",
    "print(\"RÂ²   :\", rf_r2)\n",
    "print(\"RMSE :\", rf_rmse)\n",
    "print(\"MAE  :\", rf_mae)\n",
    "\n",
    "# Hybrid\n",
    "hybrid_train_pred = (\n",
    "    best_combo[0] * pred_xgb_train +\n",
    "    best_combo[1] * pred_lgb_train +\n",
    "    best_combo[2] * pred_rf_train\n",
    ")\n",
    "\n",
    "hyb_tr_r2, hyb_tr_rmse, hyb_tr_mae = get_metrics(y_train, hybrid_train_pred)\n",
    "\n",
    "print(\"\\nðŸ”¥ Hybrid Model Train Metrics\")\n",
    "print(\"RÂ²   :\", hyb_tr_r2)\n",
    "print(\"RMSE :\", hyb_tr_rmse)\n",
    "print(\"MAE  :\", hyb_tr_mae)\n",
    "print(\"=============================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
